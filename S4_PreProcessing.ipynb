{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Overview:\n",
    "- Input: sampled_100.json \n",
    "\n",
    "- Output: preprocessed_100.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing Steps: \n",
    "- Lowercasing: Convert all text to lowercase to ensure consitency \n",
    "\n",
    "- Tokenization: Split the text into individual words or tokens.  \n",
    "\n",
    "- Removing Punctuation: Eliminate punctuation marks  \n",
    "\n",
    "- Removing Stopwords: Remove common words like \"the\", \"is\"   \n",
    "\n",
    "- Stemming or Lemmatization: Reduce words to  theirs root form  \n",
    "\n",
    "- Removing Numbers and Special Characters  \n",
    "\n",
    "- Removing Extra Whitespaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def preprocess_corpus(input_file_path, output_file_path):\n",
    "    # Open and read the JSON file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize the stopwords, stemmer, and other required objects\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Add custom stopwords\n",
    "    custom_stopwords = {'said', 'could', 'might', 'like', 'also', 'would'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "\n",
    "    # Define the preprocessing function:\n",
    "    def preprocess_text(text):\n",
    "        # Lowercasing\n",
    "        text = text.lower() \n",
    "\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text) \n",
    "\n",
    "        # Removing Punctuation\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "        # Removing Stopwords and single characters\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        # Removing Numbers and Special Characters\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Join tokens back into text\n",
    "        processed_text = ' '.join(tokens)\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    # Preprocess each story in the data, retaining only the preprocessed text\n",
    "    preprocessed_data = []\n",
    "    for story in data:\n",
    "        preprocessed_text = preprocess_text(story['story_text'])\n",
    "        preprocessed_data.append(preprocessed_text)\n",
    "\n",
    "    # Write the preprocessed data to the output file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(preprocessed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'sampled_300.json'\n",
    "output_file_path = 'tokenizedRaw_300.json'\n",
    "preprocess_corpus(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean Tokenizing by Removing MetaInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def strip_metainfo(input_file_path, output_file_path):\n",
    "    # Read the JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        stories = json.load(infile)\n",
    "    \n",
    "    # Process each story to strip meta info\n",
    "    stripped_stories = []\n",
    "    for story in stories:\n",
    "        # Find the position of the word 'body'\n",
    "        body_index = story.find('body')\n",
    "        if body_index != -1:\n",
    "            # Extract the content before the word 'body'\n",
    "            stripped_story = story[body_index + len('body'):].strip()\n",
    "            stripped_stories.append(stripped_story)\n",
    "    \n",
    "    # Write the stripped content to a new JSON file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(stripped_stories, outfile, indent=2)\n",
    "\n",
    "# Example usage\n",
    "input_file_path  = 'tokenizedRaw_300.json'\n",
    "output_file_path = 'tokenizedFine_300.json'\n",
    "strip_metainfo(input_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
