{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install required packages\n",
    "install(\"sentence-transformers\")\n",
    "install(\"bertopic\")\n",
    "install(\"pandas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 stories saved to cluster_0.txt\n",
      "123 stories saved to cluster_1.txt\n",
      "58 stories saved to cluster_2.txt\n",
      "21 stories saved to cluster_3.txt\n",
      "Topic modeling completed for all clusters.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "import pickle\n",
    "\n",
    "# TASK 1: Read the Clustered Data\n",
    "def read_clustered_data(clustered_file_path, tokenized_data_path):\n",
    "    # Load clustered data\n",
    "    clustered_data = pd.read_csv(clustered_file_path)\n",
    "\n",
    "    # Read tokenized data\n",
    "    with open(tokenized_data_path, 'r') as file:\n",
    "        tokenized_data = [line.strip().strip('\"') for line in file.readlines()]\n",
    "\n",
    "    # Map clusters to original text data\n",
    "    merged_data = pd.concat([clustered_data, pd.DataFrame(tokenized_data, columns=['text'])], axis=1)\n",
    "\n",
    "    # Group data by cluster label\n",
    "    grouped_data = merged_data.groupby('Cluster')\n",
    "\n",
    "    # Iterate over groups and access member stories\n",
    "    for cluster_label, cluster_group in grouped_data:\n",
    "        # Ensure cluster label is an integer\n",
    "        cluster_label = int(cluster_label)\n",
    "        \n",
    "        # Save clustered stories to individual files\n",
    "        cluster_file_name = f\"cluster_{cluster_label}.txt\"\n",
    "        cluster_group['text'].to_csv(cluster_file_name, index=False, header=False)\n",
    "        \n",
    "        # Print number of stories saved\n",
    "        num_stories = cluster_group.shape[0]\n",
    "        print(f\"{num_stories} stories saved to {cluster_file_name}\")\n",
    "\n",
    "# TASK 2: Perform the Documents by Topics:\n",
    "def perform_topic_modeling(cluster_directory):\n",
    "    # Load a pre-trained BERT model from sentence-transformers\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    \n",
    "    # Dictionary to store documents and embeddings by cluster\n",
    "    clustered_data = {}\n",
    "\n",
    "    # Read the cluster files and encode the documents\n",
    "    for cluster_num in range(4):\n",
    "        cluster_file = os.path.join(cluster_directory, f'cluster_{cluster_num}.txt')\n",
    "        with open(cluster_file, 'r', encoding='utf-8') as file:\n",
    "            documents = [line.strip() for line in file.readlines()]\n",
    "            embeddings = model.encode(documents)\n",
    "            clustered_data[f'cluster_{cluster_num}'] = (documents, embeddings)\n",
    "\n",
    "    # Dictionary to store topic models and topics for each cluster\n",
    "    topics_per_cluster = {}\n",
    "\n",
    "    # Perform topic modeling on the documents and embeddings for each cluster\n",
    "    for cluster_label, (documents, embeddings) in clustered_data.items():\n",
    "        topic_model = BERTopic()\n",
    "        topics, probabilities = topic_model.fit_transform(documents, embeddings)\n",
    "        topics_per_cluster[cluster_label] = {\n",
    "            'topic_model': topic_model,\n",
    "            'topics': topics,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "\n",
    "    # Save the topic models and topics for all clusters\n",
    "    with open('topics_per_cluster.pkl', 'wb') as f:\n",
    "        pickle.dump(topics_per_cluster, f)\n",
    "\n",
    "    print(\"Topic modeling completed for all clusters.\")\n",
    "\n",
    "# Example usage\n",
    "clustered_file_path = 'clustered_BERT_300.csv'  # Update with your file path\n",
    "tokenized_data_path = 'tokenizedFine_300.json'  # Update with your tokenized data file path\n",
    "cluster_directory = '.'  # Directory where the cluster files will be saved\n",
    "\n",
    "read_clustered_data(clustered_file_path, tokenized_data_path)\n",
    "perform_topic_modeling(cluster_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Showcase the Topic Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics for cluster_0:\n",
      "Topic -1: [('school', 0.033373692121562874), ('company', 0.031216665033610414), ('ai', 0.029117901302526526), ('new', 0.02767346700028996), ('year', 0.02734000623352968), ('technology', 0.02628171783550857), ('tool', 0.023919292084424035), ('time', 0.019901565581084642), ('one', 0.01989954742980924), ('chatgpt', 0.019265406462875372)]\n",
      "Topic 0: [('company', 0.03390097855673611), ('ai', 0.028230041150001575), ('new', 0.027593570930056252), ('google', 0.02579257479740746), ('technology', 0.023782677122833433), ('year', 0.023709284164320357), ('generative', 0.021080608720773032), ('one', 0.019926807181321702), ('openai', 0.019866057967612743), ('tool', 0.019394945953819773)]\n",
      "Topic 1: [('company', 0.035909256220474126), ('new', 0.03288411642871309), ('image', 0.03261165271780897), ('technology', 0.03064174947313116), ('ai', 0.027058740022584772), ('openai', 0.026101025572528205), ('microsoft', 0.02490968491208882), ('time', 0.024299148372854063), ('artificial', 0.021152880299625098), ('use', 0.02011381288186803)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_per_cluster.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m topics_per_cluster \u001b[38;5;241m=\u001b[39m load_topic_models(file_path)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mdisplay_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopics_per_cluster\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mdisplay_topics\u001b[0;34m(topics_per_cluster)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_num, topic \u001b[38;5;129;01min\u001b[39;00m topics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print top 10 words for each topic\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bertopic/_bertopic.py:2249\u001b[0m, in \u001b[0;36mBERTopic.visualize_topics\u001b[0;34m(self, topics, top_n_topics, custom_labels, title, width, height)\u001b[0m\n\u001b[1;32m   2216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Visualize topics, their sizes, and their corresponding words\u001b[39;00m\n\u001b[1;32m   2217\u001b[0m \n\u001b[1;32m   2218\u001b[0m \u001b[38;5;124;03mThis visualization is highly inspired by LDAvis, a great visualization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2248\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 2249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bertopic/plotting/_topics.py:79\u001b[0m, in \u001b[0;36mvisualize_topics\u001b[0;34m(topic_model, topics, top_n_topics, custom_labels, title, width, height)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m topic_model\u001b[38;5;241m.\u001b[39mtopic_embeddings_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mtopic_embeddings_[indices]\n\u001b[0;32m---> 79\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mUMAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mc_tf_idf_\u001b[38;5;241m.\u001b[39mtoarray()[indices]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/umap/umap_.py:2891\u001b[0m, in \u001b[0;36mUMAP.fit_transform\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2856\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[1;32m   2857\u001b[0m \u001b[38;5;124;03m    output.\u001b[39;00m\n\u001b[1;32m   2858\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;124;03m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2891\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2893\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dens:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/umap/umap_.py:2784\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2781\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2782\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[1;32m   2783\u001b[0m     )\n\u001b[0;32m-> 2784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2785\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_list\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/umap/umap_.py:2830\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state):\n\u001b[1;32m   2827\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;124;03m    replaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/umap/umap_.py:1087\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1084\u001b[0m n_epochs_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_epochs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_epochs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n_epochs\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_epochs_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m-> 1087\u001b[0m     graph\u001b[38;5;241m.\u001b[39mdata[graph\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m<\u001b[39m (\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(n_epochs_max))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     graph\u001b[38;5;241m.\u001b[39mdata[graph\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m<\u001b[39m (graph\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(default_epochs))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/numpy/core/_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_topic_models(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        topics_per_cluster = pickle.load(f)\n",
    "    return topics_per_cluster\n",
    "\n",
    "def display_topics(topics_per_cluster):\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topic_model = data['topic_model']\n",
    "        topics = topic_model.get_topics()\n",
    "        print(f\"\\nTopics for {cluster_label}:\")\n",
    "        for topic_num, topic in topics.items():\n",
    "            print(f\"Topic {topic_num}: {topic[:10]}\")  # Print top 10 words for each topic\n",
    "        \n",
    "        fig = topic_model.visualize_topics()\n",
    "        plt.title(f\"Topics for {cluster_label}\")\n",
    "        plt.show()\n",
    "\n",
    "# Load and display the topic models\n",
    "file_path = 'topics_per_cluster.pkl'\n",
    "topics_per_cluster = load_topic_models(file_path)\n",
    "display_topics(topics_per_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(topics_per_cluster):\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topic_model = data['topic_model']\n",
    "        topics = topic_model.get_topics()\n",
    "        print(f\"\\nTopics for {cluster_label}:\")\n",
    "        for topic_num, topic in topics.items():\n",
    "            print(f\"Topic {topic_num}: {topic[:10]}\")  # Print top 10 words for each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_0': {'topic_model': <bertopic._bertopic.BERTopic object at 0x1dbb2ecf0>, 'topics': [0, 1, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 1, 0, 0, 1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 1, 0, -1, 1, 1, -1, -1, -1, -1, 1, 1, 0, -1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 1, 1, 1, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 1, 0, -1, 0, -1, -1, 1, -1, 0, 1], 'probabilities': array([1.        , 1.        , 0.95181343, 0.        , 0.        ,\n",
      "       1.        , 0.91357558, 0.92984099, 0.        , 0.88086612,\n",
      "       0.        , 0.8902017 , 0.92205952, 1.        , 0.        ,\n",
      "       0.        , 1.        , 0.87904762, 0.96103823, 0.94677581,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.98936146,\n",
      "       0.96093121, 0.93236205, 1.        , 0.        , 0.90757964,\n",
      "       1.        , 0.        , 1.        , 0.85337453, 0.        ,\n",
      "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
      "       0.95897096, 0.        , 1.        , 0.        , 1.        ,\n",
      "       0.        , 0.88924498, 0.        , 0.89448689, 0.97671766,\n",
      "       1.        , 1.        , 0.        , 0.        , 0.87904762,\n",
      "       0.98732957, 0.        , 0.        , 1.        , 0.87882903,\n",
      "       1.        , 0.        , 0.85643903, 1.        , 0.        ,\n",
      "       0.91476353, 0.88953657, 0.        , 0.98820861, 0.92984099,\n",
      "       0.95987197, 0.        , 0.89018939, 0.        , 0.        ,\n",
      "       0.        , 0.90572179, 0.87784014, 0.        , 0.90206351,\n",
      "       0.92004016, 0.        , 0.        , 0.        , 1.        ,\n",
      "       1.        , 0.        , 0.89988711, 1.        , 0.90303775,\n",
      "       0.        , 1.        , 0.        , 0.        , 0.87506921,\n",
      "       0.        , 1.        , 1.        ])}, 'cluster_1': {'topic_model': <bertopic._bertopic.BERTopic object at 0x1dcae41d0>, 'topics': [1, -1, 0, -1, 0, -1, 1, 1, -1, 0, -1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 1, 0, 1, 1, -1, 1, 1, 0, 0, -1, -1, 1, 0, -1, 0, -1, 0, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 0, -1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, -1, -1, 1, 0, 0, 0, 1, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, -1, 0, 1, 1, 0, -1, 0, 0, -1, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, -1, 1, -1, -1, 1, 0, -1, 1, 0, 1], 'probabilities': array([1.        , 0.        , 1.        , 0.        , 0.7741955 ,\n",
      "       0.        , 1.        , 1.        , 0.        , 0.86117789,\n",
      "       0.        , 1.        , 1.        , 1.        , 0.80259739,\n",
      "       0.79322059, 0.97287086, 0.95922784, 1.        , 1.        ,\n",
      "       1.        , 0.        , 1.        , 0.82130949, 0.98466893,\n",
      "       1.        , 0.        , 1.        , 1.        , 0.9101076 ,\n",
      "       0.82769073, 0.        , 0.        , 0.95326746, 0.95797257,\n",
      "       0.        , 0.86540909, 0.        , 0.86540909, 0.        ,\n",
      "       1.        , 1.        , 0.        , 0.92928901, 0.        ,\n",
      "       1.        , 0.92337073, 0.        , 0.        , 0.82648505,\n",
      "       0.        , 0.        , 0.86540909, 0.        , 1.        ,\n",
      "       0.81693807, 0.        , 1.        , 0.81693807, 0.93415223,\n",
      "       1.        , 0.        , 1.        , 0.        , 0.        ,\n",
      "       0.84586546, 0.80586359, 1.        , 1.        , 1.        ,\n",
      "       0.86540909, 0.79677859, 0.        , 0.85835817, 0.80445816,\n",
      "       0.        , 0.        , 0.94551902, 0.86540909, 0.79322059,\n",
      "       0.86540909, 1.        , 0.        , 1.        , 0.        ,\n",
      "       0.85215493, 0.86540909, 0.        , 0.82769073, 1.        ,\n",
      "       1.        , 0.        , 0.        , 0.        , 0.82320054,\n",
      "       0.92337073, 1.        , 0.86103838, 0.        , 0.77699375,\n",
      "       1.        , 0.        , 0.95555255, 1.        , 0.        ,\n",
      "       0.80259739, 1.        , 1.        , 0.        , 0.97584317,\n",
      "       0.86540909, 0.78670614, 0.80812962, 0.        , 1.        ,\n",
      "       0.        , 0.        , 1.        , 0.83868636, 0.        ,\n",
      "       0.96671605, 0.80118023, 1.        ])}, 'cluster_2': {'topic_model': <bertopic._bertopic.BERTopic object at 0x1dca225d0>, 'topics': [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], 'probabilities': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.])}, 'cluster_3': {'topic_model': <bertopic._bertopic.BERTopic object at 0x1dbcd9970>, 'topics': [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], 'probabilities': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0.])}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_topic_models(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        topics_per_cluster = pickle.load(f)\n",
    "    return topics_per_cluster\n",
    "\n",
    "# Load the topic models\n",
    "file_path = 'topics_per_cluster.pkl'\n",
    "topics_per_cluster = load_topic_models(file_path)\n",
    "\n",
    "# Print loaded topics_per_cluster to verify content\n",
    "print(topics_per_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(topics_per_cluster):\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topic_model = data['topic_model']\n",
    "        topics = topic_model.get_topics()\n",
    "        print(f\"\\nTopics for {cluster_label}:\")\n",
    "        print(f\"Number of topics: {len(topics)}\")  # Print number of topics\n",
    "        for topic_num, topic in topics.items():\n",
    "            print(f\"Topic {topic_num}: {topic[:10]}\")  # Print top 10 words for each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error visualizing topics for cluster_0: zero-size array to reduction operation maximum which has no identity\n",
      "Error visualizing topics for cluster_1: zero-size array to reduction operation maximum which has no identity\n",
      "Error visualizing topics for cluster_2: arrays used as indices must be of integer (or boolean) type\n",
      "Error visualizing topics for cluster_3: arrays used as indices must be of integer (or boolean) type\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_topics(topics_per_cluster):\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topic_model = data['topic_model']\n",
    "        try:\n",
    "            fig = topic_model.visualize_topics()\n",
    "            plt.title(f\"Topics for {cluster_label}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing topics for {cluster_label}: {str(e)}\")\n",
    "\n",
    "# Assuming topics_per_cluster is already loaded from topics_per_cluster.pkl\n",
    "visualize_topics(topics_per_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: cluster_0\n",
      "Topics: [0, 1, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 1, 0, 0, 1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 1, 0, -1, 1, 1, -1, -1, -1, -1, 1, 1, 0, -1, 0, -1, 1, -1, 1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 1, 1, 1, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 1, 0, -1, 0, -1, -1, 1, -1, 0, 1]\n",
      "Probabilities: [1.         1.         0.95181343 0.         0.         1.\n",
      " 0.91357558 0.92984099 0.         0.88086612 0.         0.8902017\n",
      " 0.92205952 1.         0.         0.         1.         0.87904762\n",
      " 0.96103823 0.94677581 0.         0.         0.         0.\n",
      " 0.98936146 0.96093121 0.93236205 1.         0.         0.90757964\n",
      " 1.         0.         1.         0.85337453 0.         0.\n",
      " 0.         0.         1.         1.         0.95897096 0.\n",
      " 1.         0.         1.         0.         0.88924498 0.\n",
      " 0.89448689 0.97671766 1.         1.         0.         0.\n",
      " 0.87904762 0.98732957 0.         0.         1.         0.87882903\n",
      " 1.         0.         0.85643903 1.         0.         0.91476353\n",
      " 0.88953657 0.         0.98820861 0.92984099 0.95987197 0.\n",
      " 0.89018939 0.         0.         0.         0.90572179 0.87784014\n",
      " 0.         0.90206351 0.92004016 0.         0.         0.\n",
      " 1.         1.         0.         0.89988711 1.         0.90303775\n",
      " 0.         1.         0.         0.         0.87506921 0.\n",
      " 1.         1.        ]\n",
      "Error visualizing topics for cluster_0: zero-size array to reduction operation maximum which has no identity\n",
      "Cluster: cluster_1\n",
      "Topics: [1, -1, 0, -1, 0, -1, 1, 1, -1, 0, -1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, -1, 1, 0, 1, 1, -1, 1, 1, 0, 0, -1, -1, 1, 0, -1, 0, -1, 0, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 0, -1, -1, 0, -1, 1, 0, -1, 1, 0, 1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 1, 0, 0, -1, 0, 0, -1, -1, 1, 0, 0, 0, 1, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, -1, 0, 1, 1, 0, -1, 0, 0, -1, 0, 1, -1, 0, 1, 0, -1, 1, 0, 0, 0, -1, 1, -1, -1, 1, 0, -1, 1, 0, 1]\n",
      "Probabilities: [1.         0.         1.         0.         0.7741955  0.\n",
      " 1.         1.         0.         0.86117789 0.         1.\n",
      " 1.         1.         0.80259739 0.79322059 0.97287086 0.95922784\n",
      " 1.         1.         1.         0.         1.         0.82130949\n",
      " 0.98466893 1.         0.         1.         1.         0.9101076\n",
      " 0.82769073 0.         0.         0.95326746 0.95797257 0.\n",
      " 0.86540909 0.         0.86540909 0.         1.         1.\n",
      " 0.         0.92928901 0.         1.         0.92337073 0.\n",
      " 0.         0.82648505 0.         0.         0.86540909 0.\n",
      " 1.         0.81693807 0.         1.         0.81693807 0.93415223\n",
      " 1.         0.         1.         0.         0.         0.84586546\n",
      " 0.80586359 1.         1.         1.         0.86540909 0.79677859\n",
      " 0.         0.85835817 0.80445816 0.         0.         0.94551902\n",
      " 0.86540909 0.79322059 0.86540909 1.         0.         1.\n",
      " 0.         0.85215493 0.86540909 0.         0.82769073 1.\n",
      " 1.         0.         0.         0.         0.82320054 0.92337073\n",
      " 1.         0.86103838 0.         0.77699375 1.         0.\n",
      " 0.95555255 1.         0.         0.80259739 1.         1.\n",
      " 0.         0.97584317 0.86540909 0.78670614 0.80812962 0.\n",
      " 1.         0.         0.         1.         0.83868636 0.\n",
      " 0.96671605 0.80118023 1.        ]\n",
      "Error visualizing topics for cluster_1: zero-size array to reduction operation maximum which has no identity\n",
      "Cluster: cluster_2\n",
      "Topics: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Probabilities: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Error visualizing topics for cluster_2: arrays used as indices must be of integer (or boolean) type\n",
      "Cluster: cluster_3\n",
      "Topics: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Probabilities: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Error visualizing topics for cluster_3: arrays used as indices must be of integer (or boolean) type\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_topics(topics_per_cluster):\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topic_model = data['topic_model']\n",
    "        topics = data['topics']\n",
    "        probabilities = data['probabilities']\n",
    "        \n",
    "        try:\n",
    "            # Check topics and probabilities before visualization\n",
    "            print(f\"Cluster: {cluster_label}\")\n",
    "            print(f\"Topics: {topics}\")\n",
    "            print(f\"Probabilities: {probabilities}\")\n",
    "            \n",
    "            # Visualize topics\n",
    "            fig = topic_model.visualize_topics()\n",
    "            plt.title(f\"Topics for {cluster_label}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing topics for {cluster_label}: {str(e)}\")\n",
    "\n",
    "# Assuming topics_per_cluster is already loaded from topics_per_cluster.pkl\n",
    "visualize_topics(topics_per_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file loaded successfully.\n",
      "Keys in the loaded data: dict_keys(['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3'])\n",
      "Cluster: cluster_0\n",
      "Number of topics: 98\n",
      "Number of probabilities: 98\n",
      "Example topics: [0, 1, 1, -1, -1, 0, 0, 0, -1, 0]\n",
      "Example probabilities: [1.         1.         0.95181343 0.         0.         1.\n",
      " 0.91357558 0.92984099 0.         0.88086612]\n",
      "\n",
      "Cluster: cluster_1\n",
      "Number of topics: 123\n",
      "Number of probabilities: 123\n",
      "Example topics: [1, -1, 0, -1, 0, -1, 1, 1, -1, 0]\n",
      "Example probabilities: [1.         0.         1.         0.         0.7741955  0.\n",
      " 1.         1.         0.         0.86117789]\n",
      "\n",
      "Cluster: cluster_2\n",
      "Number of topics: 58\n",
      "Number of probabilities: 58\n",
      "Example topics: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Example probabilities: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Cluster: cluster_3\n",
      "Number of topics: 21\n",
      "Number of probabilities: 21\n",
      "Example topics: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Example probabilities: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Function to load the topic models from the pickle file\n",
    "def load_topic_models(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        topics_per_cluster = pickle.load(file)\n",
    "    return topics_per_cluster\n",
    "\n",
    "# File path to the pickle file\n",
    "file_path = 'topics_per_cluster.pkl'\n",
    "\n",
    "# Load the topic models\n",
    "try:\n",
    "    topics_per_cluster = load_topic_models(file_path)\n",
    "    print(\"Pickle file loaded successfully.\")\n",
    "    \n",
    "    # Print the keys to verify the structure\n",
    "    print(\"Keys in the loaded data:\", topics_per_cluster.keys())\n",
    "    \n",
    "    # Example: Print some information from the loaded data\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        print(f\"Cluster: {cluster_label}\")\n",
    "        print(f\"Number of topics: {len(data['topics'])}\")\n",
    "        print(f\"Number of probabilities: {len(data['probabilities'])}\")\n",
    "        print(f\"Example topics: {data['topics'][:10]}\")\n",
    "        print(f\"Example probabilities: {data['probabilities'][:10]}\")\n",
    "        print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading pickle file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: cluster_0\n",
      "Topic topic_0: 36 documents\n",
      "Topic topic_1: 22 documents\n",
      "Topic topic_-1: 0 documents\n",
      "Cluster: cluster_1\n",
      "Topic topic_0: 46 documents\n",
      "Topic topic_1: 37 documents\n",
      "Topic topic_-1: 0 documents\n",
      "Cluster: cluster_2\n",
      "Topic topic_-1: 0 documents\n",
      "Cluster: cluster_3\n",
      "Topic topic_-1: 0 documents\n"
     ]
    }
   ],
   "source": [
    "def assign_documents_to_topics(topics_per_cluster):\n",
    "    classified_documents = {}\n",
    "\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topics = data['topics']\n",
    "        probabilities = data['probabilities']\n",
    "\n",
    "        # Create a dictionary to store document classifications by topics\n",
    "        classified_documents[cluster_label] = {f'topic_{topic}': [] for topic in set(topics)}\n",
    "\n",
    "        # Iterate through documents and assign them to topics based on probabilities\n",
    "        for doc_index in range(len(probabilities)):\n",
    "            max_topic_idx = topics[doc_index]\n",
    "            if max_topic_idx != -1:\n",
    "                topic_key = f'topic_{max_topic_idx}'\n",
    "                classified_documents[cluster_label][topic_key].append(doc_index)\n",
    "\n",
    "    return classified_documents\n",
    "\n",
    "# Example usage\n",
    "classified_docs = assign_documents_to_topics(topics_per_cluster)\n",
    "\n",
    "# Print the classified documents for each cluster\n",
    "for cluster_label, topics_dict in classified_docs.items():\n",
    "    print(f\"Cluster: {cluster_label}\")\n",
    "    for topic_key, doc_indices in topics_dict.items():\n",
    "        print(f\"Topic {topic_key}: {len(doc_indices)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: cluster_0\n",
      "Topic topic_-1: [('school', 0.033373692121562874), ('company', 0.031216665033610414), ('ai', 0.029117901302526526), ('new', 0.02767346700028996), ('year', 0.02734000623352968), ('technology', 0.02628171783550857), ('tool', 0.023919292084424035), ('time', 0.019901565581084642), ('one', 0.01989954742980924), ('chatgpt', 0.019265406462875372)]\n",
      "Topic topic_0: [('company', 0.03390097855673611), ('ai', 0.028230041150001575), ('new', 0.027593570930056252), ('google', 0.02579257479740746), ('technology', 0.023782677122833433), ('year', 0.023709284164320357), ('generative', 0.021080608720773032), ('one', 0.019926807181321702), ('openai', 0.019866057967612743), ('tool', 0.019394945953819773)]\n",
      "Topic topic_1: [('company', 0.035909256220474126), ('new', 0.03288411642871309), ('image', 0.03261165271780897), ('technology', 0.03064174947313116), ('ai', 0.027058740022584772), ('openai', 0.026101025572528205), ('microsoft', 0.02490968491208882), ('time', 0.024299148372854063), ('artificial', 0.021152880299625098), ('use', 0.02011381288186803)]\n",
      "Cluster: cluster_1\n",
      "Topic topic_-1: [('ai', 0.03546244540260965), ('company', 0.031309330697299645), ('apple', 0.02524423470010913), ('new', 0.02480928455300536), ('google', 0.022271904976658763), ('year', 0.021543789164750863), ('technology', 0.019631520083172555), ('generative', 0.017150914002374184), ('india', 0.017054582618377133), ('data', 0.016917428713123364)]\n",
      "Topic topic_0: [('company', 0.031270099864802454), ('technology', 0.030189042755368403), ('new', 0.027088460672530783), ('openai', 0.02680921447947811), ('time', 0.023735792000246603), ('ai', 0.021255698401666686), ('google', 0.020653093604320594), ('people', 0.0202874032370233), ('one', 0.019696109413307506), ('intelligence', 0.017848400943736277)]\n",
      "Topic topic_1: [('year', 0.037017034501257415), ('ai', 0.03612830463721428), ('company', 0.03363405975143703), ('metaverse', 0.02609049673969627), ('new', 0.02558025376221878), ('technology', 0.02455442385707205), ('china', 0.02442973543996782), ('generative', 0.02352240184495559), ('billion', 0.021896261301224004), ('world', 0.019390052972867146)]\n",
      "Cluster: cluster_2\n",
      "Topic topic_-1: [('company', 0.038198420284233786), ('ai', 0.03458191660856269), ('new', 0.02931125453508708), ('technology', 0.025370772954298893), ('one', 0.024832620625982686), ('people', 0.022646581896603818), ('generative', 0.021812146185845573), ('intelligence', 0.020827758998777254), ('year', 0.02040213529621379), ('time', 0.020259746184595445)]\n",
      "Cluster: cluster_3\n",
      "Topic topic_-1: [('company', 0.07354154943668899), ('microsoft', 0.0558972498694438), ('ai', 0.038960260630600325), ('google', 0.03648280628540389), ('tech', 0.034589383475087915), ('union', 0.03004018431031731), ('year', 0.028703089961623567), ('employee', 0.02700580590960198), ('time', 0.026662716576016834), ('worker', 0.025625831878925316)]\n"
     ]
    }
   ],
   "source": [
    "def get_topic_keywords(topics_per_cluster):\n",
    "    topic_keywords = {}\n",
    "\n",
    "    for cluster_label, data in topics_per_cluster.items():\n",
    "        topics = data['topics']\n",
    "        topic_model = data['topic_model']  # Assuming you have access to the BERTopic object\n",
    "\n",
    "        # Get topics with keywords from BERTopic\n",
    "        topics_keywords = topic_model.get_topics()\n",
    "\n",
    "        # Store topics and keywords\n",
    "        topic_keywords[cluster_label] = {\n",
    "            f'topic_{topic_num}': keywords[:10]  # Assuming you want to get top 10 keywords per topic\n",
    "            for topic_num, keywords in topics_keywords.items() if topic_num in set(topics)\n",
    "        }\n",
    "\n",
    "    return topic_keywords\n",
    "\n",
    "# Example usage\n",
    "topic_keywords = get_topic_keywords(topics_per_cluster)\n",
    "\n",
    "# Print or use topic_keywords dictionary as needed\n",
    "for cluster_label, topics_data in topic_keywords.items():\n",
    "    print(f\"Cluster: {cluster_label}\")\n",
    "    for topic_key, keywords in topics_data.items():\n",
    "        print(f\"Topic {topic_key}: {keywords}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
