{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text Preview:\n",
      "amazon offer fulfilment infrastructure seller outside platform economic time september friday copyright bennett coleman right reserved section tech internet length word byline annapurna roy body amazon thursday announced launch new product multichannel fulfilment enable seller run online business accessing fulfilment infrastructure even platform excited announce product called multichannel fulfilment allows small business brand take advantage fulfilment infrastructure worry inventory fulfilment \n",
      "\n",
      "Top 10 2-grams:\n",
      "new york: 561\n",
      "artificial intelligence: 514\n",
      "york time: 461\n",
      "length word: 300\n",
      "word byline: 279\n",
      "right reserved: 252\n",
      "reserved section: 225\n",
      "generative ai: 210\n",
      "copyright new: 158\n",
      "time company: 154\n",
      "\n",
      "Top 10 3-grams:\n",
      "new york time: 461\n",
      "length word byline: 279\n",
      "right reserved section: 225\n",
      "copyright new york: 158\n",
      "york time company: 146\n",
      "article appeared print: 100\n",
      "appeared print page: 100\n",
      "generative artificial intelligence: 93\n",
      "company right reserved: 90\n",
      "pg length word: 84\n",
      "\n",
      "Top 10 4-grams:\n",
      "new york time company: 146\n",
      "copyright new york time: 145\n",
      "article appeared print page: 100\n",
      "company right reserved section: 90\n",
      "est copyright new york: 81\n",
      "york time company right: 81\n",
      "time company right reserved: 81\n",
      "pg length word byline: 81\n",
      "copyright bennett coleman right: 74\n",
      "bennett coleman right reserved: 74\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_top_ngrams(input_file_path):\n",
    "    # Load the preprocessed news corpus\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        preprocessed_data = json.load(file)\n",
    "    \n",
    "    # Combine all preprocessed text into a single string\n",
    "    all_text = ' '.join(preprocessed_data)\n",
    "    \n",
    "    # Debug statement to check the combined text\n",
    "    print(\"Combined Text Preview:\")\n",
    "    print(all_text[:500])  # Print the first 500 characters of the combined text\n",
    "\n",
    "    # Function to extract n-grams and count their frequencies\n",
    "    def extract_ngrams(text, n, top_k):\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate n-grams\n",
    "        n_grams = ngrams(words, n)\n",
    "        # Count the frequencies of each n-gram\n",
    "        n_gram_freq = Counter(n_grams)\n",
    "        # Get the top k n-grams by frequency\n",
    "        top_ngrams = n_gram_freq.most_common(top_k)\n",
    "        return top_ngrams\n",
    "\n",
    "    # Define the range of n-grams and the number of top n-grams to retrieve\n",
    "    n_values = [2, 3, 4]\n",
    "    top_k = 10\n",
    "\n",
    "    # Find and print the top n-grams for each n\n",
    "    for n in n_values:\n",
    "        top_ngrams = extract_ngrams(all_text, n, top_k)\n",
    "        print(f\"\\nTop {top_k} {n}-grams:\")\n",
    "        for ngram, freq in top_ngrams:\n",
    "            print(f\"{' '.join(ngram)}: {freq}\")\n",
    "\n",
    "# Example usage\n",
    "get_top_ngrams('tokenized_300.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
