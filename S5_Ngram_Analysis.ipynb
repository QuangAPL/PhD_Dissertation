{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text Preview:\n",
      "amazon thursday announced launch new product multichannel fulfilment enable seller run online business accessing fulfilment infrastructure even platform excited announce product called multichannel fulfilment allows small business brand take advantage fulfilment infrastructure worry inventory fulfilment look irrespective whether selling amazon amazon matter digital seller run operation using amazon fulfilment infrastructure amit agarwal senior vice president emerging market amazon speaking amazo\n",
      "\n",
      "Top 30 2-grams:\n",
      "artificial intelligence: 470\n",
      "new york: 241\n",
      "generative ai: 203\n",
      "york time: 168\n",
      "chief executive: 149\n",
      "united state: 115\n",
      "last year: 112\n",
      "language model: 105\n",
      "tech company: 101\n",
      "article appeared: 100\n",
      "appeared print: 100\n",
      "print page: 100\n",
      "social medium: 91\n",
      "generative artificial: 88\n",
      "large language: 79\n",
      "last month: 73\n",
      "high school: 71\n",
      "tech giant: 66\n",
      "san francisco: 66\n",
      "search engine: 63\n",
      "silicon valley: 62\n",
      "new technology: 57\n",
      "year ago: 56\n",
      "sam altman: 51\n",
      "last week: 48\n",
      "next year: 48\n",
      "app store: 46\n",
      "reprint right: 41\n",
      "ai model: 41\n",
      "image generator: 41\n",
      "\n",
      "Top 30 3-grams:\n",
      "new york time: 168\n",
      "article appeared print: 100\n",
      "appeared print page: 100\n",
      "generative artificial intelligence: 88\n",
      "large language model: 79\n",
      "artificial intelligence ai: 31\n",
      "york time article: 28\n",
      "time article appeared: 27\n",
      "artificial intelligence technology: 24\n",
      "child sexual abuse: 23\n",
      "http graphic photo: 22\n",
      "venture capital firm: 20\n",
      "sexual abuse material: 19\n",
      "article originally appeared: 17\n",
      "graphic article appeared: 17\n",
      "federal trade commission: 17\n",
      "artificial intelligence tool: 17\n",
      "http graphic article: 16\n",
      "time sued openai: 15\n",
      "print page june: 15\n",
      "generative ai model: 14\n",
      "wall street journal: 14\n",
      "generative ai tool: 14\n",
      "originally appeared usa: 13\n",
      "appeared usa today: 13\n",
      "use artificial intelligence: 13\n",
      "using artificial intelligence: 13\n",
      "york time sued: 12\n",
      "altman chief executive: 12\n",
      "print page february: 12\n",
      "\n",
      "Top 30 4-grams:\n",
      "article appeared print page: 100\n",
      "new york time article: 28\n",
      "york time article appeared: 27\n",
      "time article appeared print: 27\n",
      "child sexual abuse material: 18\n",
      "graphic article appeared print: 17\n",
      "http graphic article appeared: 16\n",
      "appeared print page june: 15\n",
      "article originally appeared usa: 13\n",
      "originally appeared usa today: 13\n",
      "new york time sued: 12\n",
      "york time sued openai: 12\n",
      "appeared print page february: 12\n",
      "appeared print page march: 12\n",
      "sam altman chief executive: 11\n",
      "appeared print page may: 11\n",
      "large language model llm: 10\n",
      "feedback please email thought: 10\n",
      "please email thought suggestion: 10\n",
      "email thought suggestion dealbook: 10\n",
      "thought suggestion dealbook photo: 10\n",
      "high school story recent: 10\n",
      "school story recent new: 10\n",
      "story recent new york: 10\n",
      "recent new york time: 10\n",
      "new york time reporting: 10\n",
      "york time reporting secondary: 10\n",
      "time reporting secondary schooling: 10\n",
      "reporting secondary schooling teaching: 10\n",
      "secondary schooling teaching resource: 10\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_top_ngrams(input_file_path):\n",
    "    # Load the preprocessed news corpus\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        preprocessed_data = json.load(file)\n",
    "    \n",
    "    # Combine all preprocessed text into a single string\n",
    "    all_text = ' '.join(preprocessed_data)\n",
    "    \n",
    "    # Debug statement to check the combined text\n",
    "    print(\"Combined Text Preview:\")\n",
    "    print(all_text[:500])  # Print the first 500 characters of the combined text\n",
    "\n",
    "    # Function to extract n-grams and count their frequencies\n",
    "    def extract_ngrams(text, n, top_k):\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate n-grams\n",
    "        n_grams = ngrams(words, n)\n",
    "        # Count the frequencies of each n-gram\n",
    "        n_gram_freq = Counter(n_grams)\n",
    "        # Get the top k n-grams by frequency\n",
    "        top_ngrams = n_gram_freq.most_common(top_k)\n",
    "        return top_ngrams\n",
    "\n",
    "    # Define the range of n-grams and the number of top n-grams to retrieve\n",
    "    n_values = [2, 3, 4]\n",
    "    top_k = 30\n",
    "\n",
    "    # Find and print the top n-grams for each n\n",
    "    for n in n_values:\n",
    "        top_ngrams = extract_ngrams(all_text, n, top_k)\n",
    "        print(f\"\\nTop {top_k} {n}-grams:\")\n",
    "        for ngram, freq in top_ngrams:\n",
    "            print(f\"{' '.join(ngram)}: {freq}\")\n",
    "\n",
    "# Example usage\n",
    "get_top_ngrams('tokenizedFine_300.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
