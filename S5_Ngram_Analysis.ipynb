{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text Preview:\n",
      "bengaluru infosys solid foundation grow back large deal bagged fiscal year ended march nonexecutive chairman nandan nilekani large deal valued billion year net new deal promise solid foundation grow build resilience month come nilekani wednesday annual general meeting india second largest company virtual address shareholder bengalurubased firm capability harness opportunity despite dynamic time inflation interest rate geopolitics war demand volatility supply chain dislocation tremendous infosys \n",
      "\n",
      "Top 30 2-grams:\n",
      "artificial intelligence: 748\n",
      "generative ai: 417\n",
      "new york: 366\n",
      "last year: 261\n",
      "york time: 257\n",
      "chief executive: 235\n",
      "ezra klein: 227\n",
      "language model: 176\n",
      "article appeared: 173\n",
      "appeared print: 173\n",
      "print page: 173\n",
      "social medium: 158\n",
      "tech company: 158\n",
      "search engine: 135\n",
      "large language: 134\n",
      "united state: 134\n",
      "generative artificial: 118\n",
      "new technology: 109\n",
      "silicon valley: 101\n",
      "last month: 97\n",
      "tech giant: 97\n",
      "san francisco: 94\n",
      "sam altman: 90\n",
      "year ago: 87\n",
      "ai tool: 85\n",
      "vice president: 81\n",
      "reprint right: 81\n",
      "http graphic: 80\n",
      "graphic photo: 75\n",
      "wall street: 71\n",
      "\n",
      "Top 30 3-grams:\n",
      "new york time: 257\n",
      "article appeared print: 173\n",
      "appeared print page: 173\n",
      "large language model: 131\n",
      "generative artificial intelligence: 115\n",
      "artificial intelligence ai: 65\n",
      "ezra klein show: 57\n",
      "york time article: 56\n",
      "time article appeared: 56\n",
      "http graphic photo: 54\n",
      "transcript ezra klein: 54\n",
      "ezra klein interview: 54\n",
      "wall street journal: 34\n",
      "artificial intelligence technology: 33\n",
      "print page march: 32\n",
      "artificial intelligence tool: 32\n",
      "use artificial intelligence: 31\n",
      "venture capital firm: 30\n",
      "generative ai tool: 30\n",
      "article originally appeared: 27\n",
      "feedback please email: 26\n",
      "please email thought: 26\n",
      "email thought suggestion: 26\n",
      "thought suggestion dealbook: 26\n",
      "chief technology officer: 24\n",
      "print page april: 24\n",
      "social medium platform: 23\n",
      "child sexual abuse: 23\n",
      "graphic article appeared: 23\n",
      "originally appeared usa: 22\n",
      "\n",
      "Top 30 4-grams:\n",
      "article appeared print page: 173\n",
      "new york time article: 56\n",
      "york time article appeared: 56\n",
      "time article appeared print: 56\n",
      "transcript ezra klein interview: 54\n",
      "appeared print page march: 32\n",
      "feedback please email thought: 26\n",
      "please email thought suggestion: 26\n",
      "email thought suggestion dealbook: 26\n",
      "appeared print page april: 24\n",
      "graphic article appeared print: 23\n",
      "article originally appeared usa: 22\n",
      "originally appeared usa today: 22\n",
      "casey newton kevin roose: 21\n",
      "http graphic article appeared: 20\n",
      "appeared print page may: 20\n",
      "ezra klein interview ethan: 19\n",
      "klein interview ethan mollick: 19\n",
      "interview ethan mollick ezra: 19\n",
      "ethan mollick ezra klein: 19\n",
      "mollick ezra klein show: 19\n",
      "ezra klein interview casey: 18\n",
      "klein interview casey newton: 18\n",
      "interview casey newton kevin: 18\n",
      "newton kevin roose ezra: 18\n",
      "kevin roose ezra klein: 18\n",
      "roose ezra klein show: 18\n",
      "new york time sued: 17\n",
      "large language model llm: 17\n",
      "ezra klein interview alondra: 17\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_top_ngrams(input_file_path):\n",
    "    # Load the preprocessed news corpus\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        preprocessed_data = json.load(file)\n",
    "    \n",
    "    # Combine all preprocessed text into a single string\n",
    "    all_text = ' '.join(preprocessed_data)\n",
    "    \n",
    "    # Debug statement to check the combined text\n",
    "    print(\"Combined Text Preview:\")\n",
    "    print(all_text[:500])  # Print the first 500 characters of the combined text\n",
    "\n",
    "    # Function to extract n-grams and count their frequencies\n",
    "    def extract_ngrams(text, n, top_k):\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate n-grams\n",
    "        n_grams = ngrams(words, n)\n",
    "        # Count the frequencies of each n-gram\n",
    "        n_gram_freq = Counter(n_grams)\n",
    "        # Get the top k n-grams by frequency\n",
    "        top_ngrams = n_gram_freq.most_common(top_k)\n",
    "        return top_ngrams\n",
    "\n",
    "    # Define the range of n-grams and the number of top n-grams to retrieve\n",
    "    n_values = [2, 3, 4]\n",
    "    top_k = 30\n",
    "\n",
    "    # Find and print the top n-grams for each n\n",
    "    for n in n_values:\n",
    "        top_ngrams = extract_ngrams(all_text, n, top_k)\n",
    "        print(f\"\\nTop {top_k} {n}-grams:\")\n",
    "        for ngram, freq in top_ngrams:\n",
    "            print(f\"{' '.join(ngram)}: {freq}\")\n",
    "\n",
    "# Example usage\n",
    "get_top_ngrams('tokenizedFine_500.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
