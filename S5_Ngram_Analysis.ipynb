{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text Preview:\n",
      "info ink large deal new economic time paper edition june thursday mumbai edition copyright bennett coleman co ltd right reserved section startup tech length word bureau highlight nile say firm solid foundation grow build re month come body bengal info solid foundation grow back large deal bag fiscal year ended march none chairman nan nile large deal valued billion year net new deal promise solid foundation grow build re month come nile wednesday annual general meeting india second largest compan\n",
      "\n",
      "Top 30 2-grams:\n",
      "new york: 878\n",
      "artificial intelligence: 847\n",
      "york time: 726\n",
      "length word: 500\n",
      "load date: 500\n",
      "genus ai: 446\n",
      "right reserved: 399\n",
      "reserved section: 354\n",
      "last year: 263\n",
      "copyright new: 247\n",
      "time company: 241\n",
      "chief executive: 238\n",
      "ezra klein: 238\n",
      "language model: 187\n",
      "article appeared: 173\n",
      "appeared print: 173\n",
      "print page: 173\n",
      "social medium: 167\n",
      "tech company: 164\n",
      "economic time: 154\n",
      "page load: 152\n",
      "copyright bennett: 146\n",
      "bennett coleman: 146\n",
      "coleman co: 146\n",
      "co ltd: 146\n",
      "ltd right: 146\n",
      "large language: 146\n",
      "search engine: 143\n",
      "pg length: 139\n",
      "company right: 137\n",
      "\n",
      "Top 30 3-grams:\n",
      "new york time: 726\n",
      "right reserved section: 354\n",
      "copyright new york: 247\n",
      "york time company: 230\n",
      "article appeared print: 173\n",
      "appeared print page: 173\n",
      "page load date: 152\n",
      "print page load: 151\n",
      "copyright bennett coleman: 146\n",
      "bennett coleman co: 146\n",
      "coleman co ltd: 146\n",
      "co ltd right: 146\n",
      "ltd right reserved: 146\n",
      "large language model: 143\n",
      "pg length word: 139\n",
      "company right reserved: 134\n",
      "genus artificial intelligence: 128\n",
      "est copyright new: 117\n",
      "time company right: 117\n",
      "time company section: 88\n",
      "company section section: 88\n",
      "http www ny: 87\n",
      "www ny com: 87\n",
      "com load date: 87\n",
      "late edition final: 86\n",
      "edition final copyright: 86\n",
      "final copyright new: 86\n",
      "section section column: 83\n",
      "reprint right time: 81\n",
      "right time com: 81\n",
      "\n",
      "Top 30 4-grams:\n",
      "new york time company: 230\n",
      "copyright new york time: 228\n",
      "article appeared print page: 173\n",
      "appeared print page load: 151\n",
      "print page load date: 151\n",
      "copyright bennett coleman co: 146\n",
      "bennett coleman co ltd: 146\n",
      "coleman co ltd right: 146\n",
      "co ltd right reserved: 146\n",
      "ltd right reserved section: 146\n",
      "company right reserved section: 134\n",
      "est copyright new york: 117\n",
      "york time company right: 117\n",
      "time company right reserved: 117\n",
      "york time company section: 88\n",
      "time company section section: 88\n",
      "http www ny com: 87\n",
      "late edition final copyright: 86\n",
      "edition final copyright new: 86\n",
      "final copyright new york: 86\n",
      "company section section column: 83\n",
      "reprint right time com: 81\n",
      "time com load date: 78\n",
      "right time com load: 73\n",
      "desk pg length word: 70\n",
      "business financial desk pg: 66\n",
      "economic time paper edition: 65\n",
      "edition copyright bennett coleman: 65\n",
      "section section column business: 64\n",
      "section column business financial: 64\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_top_ngrams(input_file_path):\n",
    "    # Load the preprocessed news corpus\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        preprocessed_data = json.load(file)\n",
    "    \n",
    "    # Combine all preprocessed text into a single string\n",
    "    all_text = ' '.join(preprocessed_data)\n",
    "    \n",
    "    # Debug statement to check the combined text\n",
    "    print(\"Combined Text Preview:\")\n",
    "    print(all_text[:500])  # Print the first 500 characters of the combined text\n",
    "\n",
    "    # Function to extract n-grams and count their frequencies\n",
    "    def extract_ngrams(text, n, top_k):\n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate n-grams\n",
    "        n_grams = ngrams(words, n)\n",
    "        # Count the frequencies of each n-gram\n",
    "        n_gram_freq = Counter(n_grams)\n",
    "        # Get the top k n-grams by frequency\n",
    "        top_ngrams = n_gram_freq.most_common(top_k)\n",
    "        return top_ngrams\n",
    "\n",
    "    # Define the range of n-grams and the number of top n-grams to retrieve\n",
    "    n_values = [2, 3, 4]\n",
    "    top_k = 30\n",
    "\n",
    "    # Find and print the top n-grams for each n\n",
    "    for n in n_values:\n",
    "        top_ngrams = extract_ngrams(all_text, n, top_k)\n",
    "        print(f\"\\nTop {top_k} {n}-grams:\")\n",
    "        for ngram, freq in top_ngrams:\n",
    "            print(f\"{' '.join(ngram)}: {freq}\")\n",
    "\n",
    "# Example usage\n",
    "get_top_ngrams('tokenizedBERT_500.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
