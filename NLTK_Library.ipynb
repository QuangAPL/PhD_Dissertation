{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Download the Data Manually\n",
    "- Download the neccessary NLTK data files ('punkt, stopwords, etc) from the \n",
    "  NLTK data website:\n",
    "  - NLTK Data: https://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Extract the Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "downloads_dir = '/Users/QuangAP/Downloads' # Path of the downloads directory\n",
    "nltk_data_dir = '/Users/QuangAP/nltk_data'\n",
    "\n",
    "# List of files to extract\n",
    "files_to_extract = ['punkt.zip', 'stopwords.zip', 'sentiwordnet.zip', \n",
    "                    'words.zip', 'wordnet.zip', 'wordnet31.zip']\n",
    "\n",
    "for file_name in files_to_extract:\n",
    "    file_path = os.path.join(downloads_dir, file_name)\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Move the 'stopwords' and 'punkt' folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths\n",
    "nltk_data_dir = '/Users/QuangAP/nltk_data'\n",
    "corpora_dir = os.path.join(nltk_data_dir, 'corpora')\n",
    "tokenizers_dir = os.path.join(nltk_data_dir, 'tokenizers')\n",
    "\n",
    "# Create the necessary directories if they don't exist\n",
    "os.makedirs(corpora_dir, exist_ok=True)\n",
    "os.makedirs(tokenizers_dir, exist_ok=True)\n",
    "\n",
    "# Move the stopwords folder to the corpora directory\n",
    "stopwords_src = os.path.join(nltk_data_dir, 'stopwords')\n",
    "stopwords_dst = os.path.join(corpora_dir, 'stopwords')\n",
    "if os.path.exists(stopwords_src):\n",
    "    shutil.move(stopwords_src, stopwords_dst)\n",
    "\n",
    "# Move the punkt folder to the tokenizers directory\n",
    "punkt_src = os.path.join(nltk_data_dir, 'punkt')\n",
    "punkt_dst = os.path.join(tokenizers_dir, 'punkt')\n",
    "if os.path.exists(punkt_src):\n",
    "    shutil.move(punkt_src, punkt_dst)\n",
    "\n",
    "print(\"Files moved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Verify the Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords loaded successfully!\n",
      "Sample stopwords: [\"you've\", 'as', 'shan', \"hasn't\", 'after', 'than', 'can', 'each', 'such', 'hasn']\n",
      "Filtered text: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "Punkt and stopwords datasets are being used from the local directory.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Append the custom nltk_data directory to the nltk data path\n",
    "nltk.data.path.append('/Users/QuangAP/nltk_data')\n",
    "\n",
    "# Verify that stopwords and punkt are loaded correctly\n",
    "try:\n",
    "    # Load stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    print(\"Stopwords loaded successfully!\")\n",
    "    print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "    \n",
    "    # Tokenize a sample text\n",
    "    text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "    print(f\"Filtered text: {filtered_text}\")\n",
    "    \n",
    "    print(\"Punkt and stopwords datasets are being used from the local directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren', 'they', 'off', \"weren't\", 'wasn', 'down', 'on', 'an', \"it's\", 'our', 'with', 'about', 'shouldn', 'didn', 'a', 'below', \"isn't\", 'needn', 'doing', 'i', 'myself', \"don't\", 'can', 'in', 're', 'are', 'few', 'up', 'all', 'through', \"you're\", 'him', 'not', 'just', 'he', 'll', 'so', \"she's\", \"that'll\", 'those', 'and', 'being', 'how', 'yourselves', \"wouldn't\", 'is', 'as', 'was', 'same', 'which', 'has', \"hasn't\", 'mustn', 'there', 'for', \"doesn't\", 'them', \"didn't\", 'her', 'were', 'his', 'o', 'mightn', 'who', \"hadn't\", 'further', 'some', 'any', 'been', \"shan't\", 'did', \"needn't\", 'above', 'here', 't', 'your', 'it', 'while', 'what', 'other', 'you', 'ourselves', 'd', 'that', 'too', 'into', 'once', 'very', \"shouldn't\", 'each', 'have', 'm', 'haven', \"couldn't\", 'hadn', 'doesn', 'to', 'we', 'am', 'most', \"haven't\", 'than', 'why', 'such', 'both', 'if', 'won', 'himself', 'ain', 'my', 'shan', 'don', 'herself', 'then', 'no', 'before', 'y', 'she', 'couldn', 'should', 'whom', 'more', 'ma', \"you'd\", 'when', 'between', 'but', 'this', \"won't\", 'themselves', \"you'll\", 'will', 'wouldn', 'ours', \"you've\", 'be', 'yourself', 'now', 'does', 'their', 'these', 'isn', 'during', \"wasn't\", 'own', 'again', \"mustn't\", 'yours', 'do', 've', 'nor', 'having', 'where', 'by', 'itself', 'after', 'hers', 'only', \"mightn't\", 'under', 'at', 'against', \"aren't\", 'me', 'its', 's', 'hasn', 'or', 'because', 'weren', 'the', 'of', \"should've\", 'from', 'over', 'had', 'theirs', 'until', 'out'}\n"
     ]
    }
   ],
   "source": [
    "stopwords =set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the NLTK Library\n",
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'cloc\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Athur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence =\"At eight o'cloc on Thursday morning Athur didn't feel very good.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "# Lowercasing\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_text = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "[['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General', 'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed', 'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from', 'the', 'day', 'it', 'convened', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# List available categories (genres)\n",
    "categories = brown.categories()\n",
    "print(categories)\n",
    "\n",
    "# Get words from a specific category\n",
    "words = brown.words(categories='news')\n",
    "print(words[:20])\n",
    "\n",
    "# Get sentences from a specific category\n",
    "sentences = brown.sents(categories='editorial')\n",
    "print(sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Get all words from the corpus\n",
    "all_words = brown.words()\n",
    "\n",
    "# Calculate frequency distribution\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# Print the 10 most common words\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing 'stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence , showing stop words filtration .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$:\t-1.5\t0.\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append(\"/Users/QuangAP/nltk_data\")\n",
    "vader_lexicon = nltk.data.load(\"/Users/QuangAP/nltk_data/vader_lexicon/vader_lexicon.txt\")\n",
    "print(vader_lexicon[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment scores:  {'neg': 0.0, 'neu': 0.549, 'pos': 0.451, 'compound': 0.5079}\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "lexicon_score = \"/Users/QuangAP/nltk_data/vader_lexicon/vader_lexicon.txt\"\n",
    "sid = SentimentIntensityAnalyzer(lexicon_score)\n",
    "\n",
    "text = \"Professor Quang is so cool!\"\n",
    "scores = sid.polarity_scores(text)\n",
    "\n",
    "print(\"Sentiment scores: \", scores)\n",
    "\n",
    "# Interpret the sentiment\n",
    "if scores['compound'] >= 0.5:\n",
    "    print(\"Positive\")\n",
    "elif scores['compound'] <= -0.5:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['running', 'jumps', 'easily', 'fairly']\n",
      "Stemed words:  ['run', 'jump', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Example words to stem\n",
    "words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Stem the words\n",
    "stems = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words: \", words)\n",
    "print(\"Stemed words: \", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['running', 'jumps', 'easily', 'fairly']\n",
      "Stemed words:  ['run', 'jump', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Create an instance of SnowballStemer for English\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Example words to stem\n",
    "words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Stem the words\n",
    "stems = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words: \", words)\n",
    "print(\"Stemed words: \", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words:  ['running', 'jumps', 'easily', 'fairly']\n",
      "Stemed words:  ['run', 'jump', 'easy', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Create an instance of LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# Example words to stem\n",
    "words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "\n",
    "# Stem the words\n",
    "stems = [lancaster_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words: \", words)\n",
    "print(\"Stemed words: \", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence showing off the stop words filtration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter out punctuation tokens\n",
    "    words = [word for word in tokens if word.isalnum()]\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration!\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Numbers and Special Characters\n",
    "- Using Regular Expression with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a sample sentence, showing off the stop words filtration! It has numbers like 123 and special characters like @#!\n",
      "Cleaned text: This is a sample sentence showing off the stop words filtration It has numbers like and special characters like\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def remove_numbers_and_special_characters(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter out numbers and special characters\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Join the filtered tokens back into a single string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration! It has numbers like 123 and special characters like @#!\"\n",
    "cleaned_text = remove_numbers_and_special_characters(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using NLTK with POS Tagging to Keep Only Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a sample sentence, showing off the stop words filtration! It has numbers like 123 and special characters like @#!\n",
      "Cleaned text: This is a sample sentence showing off the stop words filtration It has numbers like and special characters like\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/QuangAP/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_numbers_and_special_characters(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter out tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "text = \"This is a sample sentence, showing off the stop words filtration! It has numbers like 123 and special characters like @#!\"\n",
    "cleaned_text = remove_numbers_and_special_characters(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Cleaned text:\", cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
