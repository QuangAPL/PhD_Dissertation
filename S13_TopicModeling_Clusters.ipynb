{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized cluster saved to cluster0_vectorized.csv\n",
      "Vectorized cluster saved to cluster1_vectorized.csv\n",
      "Vectorized cluster saved to cluster2_vectorized.csv\n",
      "Vectorized cluster saved to cluster3_vectorized.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_tfidf(input_file_paths, output_file_paths, max_features=5000):\n",
    "    for input_file_path, output_file_path in zip(input_file_paths, output_file_paths):\n",
    "        # Load the tokenized data\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            tokenized_data = json.load(file)\n",
    "        \n",
    "        # Assume tokenized_data is a list of strings\n",
    "        # Initialize the TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "        # Vectorize the tokenized text\n",
    "        tfidf_matrix = vectorizer.fit_transform(tokenized_data)\n",
    "\n",
    "        # Convert to a dense matrix (optional, depending on what you need next)\n",
    "        tfidf_dense = tfidf_matrix.todense()\n",
    "\n",
    "        # Get feature names\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Save as a DataFrame\n",
    "        df_tfidf = pd.DataFrame(tfidf_dense, columns=feature_names)\n",
    "        df_tfidf.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Vectorized cluster saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_paths = ['cluster_0.json', 'cluster_1.json', 'cluster_2.json', 'cluster_3.json']\n",
    "output_file_paths = ['cluster0_vectorized.csv', 'cluster1_vectorized.csv', 'cluster2_vectorized.csv', 'cluster3_vectorized.csv']\n",
    "vectorize_tfidf(input_file_paths, output_file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assigning Topics to Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics for Cluster 0:\n",
      "Cluster 0 - Topic 0:\n",
      "gemini launch openai deepmind bard took wednesday microsoft google division pro startup alphabet capable leap debate intensify upping behave peril released advanced collins stake reporter chatbot pichai sundar file board\n",
      "Cluster 0 - Topic 1:\n",
      "epic carlin apple store special game george app sector dudesy comedy kotak european comedian law competing mutual stunt core fund ipo rally seen growth rule satellite earnings account generated sweeney\n",
      "Cluster 0 - Topic 2:\n",
      "google company raghavan search language microsoft firm dealbook monday pure sport query technology arctos bridgewater public testing text fund openai team bard lamda product smaller executive chatbot chatgpt release chatbots\n",
      "Cluster 0 - Topic 3:\n",
      "google francis ai pope image detection risk llm blogpost policy schumer doronichev video content search comment tool society walker framework text chatgpt rule access openai detect people midjourney experience time\n",
      "Cluster 0 - Topic 4:\n",
      "reben medical artist actor patient art physician writer union care doctor agent realtor contract health commission new say center voice technology american studio estate visit openai economic real percent residence\n",
      "Cluster 0 - Topic 5:\n",
      "magician spirit scientific alien leap magic beyond touch endeavor golem magical old serious consciousness story indistinguishable reaching planet djinn paranormal object incantation argue impulse douthat ross embodied mysterious aaronson spiritual\n",
      "Cluster 0 - Topic 6:\n",
      "ai company india market one capital photo million product investment portfolio google semiconductor make learning genai bank model raised think year agility people global wood viswanathan disprz strategic skilling technology\n",
      "Cluster 0 - Topic 7:\n",
      "galaxy salle samsung actor yohana artist union event painting drescher art plus ai assistant mother performer task member board series algorithm image david hollywood painter deal suggest vote gain voted\n",
      "\n",
      "Topics for Cluster 1:\n",
      "Cluster 1 - Topic 0:\n",
      "guardian kapture capitalise vikas garg bateson saas oriented cooperate posed polling licensing aggregating blame resulted crossed accident positioned placed strengthen experiencing inappropriate hosted murder speculate reader unaware specify advisor bolster\n",
      "Cluster 1 - Topic 1:\n",
      "cognizant chrome masimo laion corpus saas kiani garg minor mullet cheque oxygen template swiftkey orwall depiction oximeter smartwatches oximetry girl mathrubootham issaquah nudification classmate capitalize incident prohibit teenage perpetrator identifiable\n",
      "Cluster 1 - Topic 2:\n",
      "ai company technology new india year openai google chatgpt microsoft image apple people news tool time system business use generative billion one intelligence data model work artificial amazon china tech\n",
      "Cluster 1 - Topic 3:\n",
      "sutskever contract pi tentative cohen kabatznik citation ravinutala ratification drescher banker suleyman studio balasubramaniyan pachocki schwartz fixer scammer schmidt forbes grateful lawrence costume rosie touched idled stone veteran buddhism soundstage\n",
      "Cluster 1 - Topic 4:\n",
      "airline air ernie grammarly broadcom airbus adtech aircraft fec lilly fisker wilson pricewaterhousecoopers iowa recruiting tata soaring stevens delivery water column moines fleet vr ar bottle aerospace mounjaro diabetes obesity\n",
      "Cluster 1 - Topic 5:\n",
      "gcc ftc agility humanoid russo copywriter rubio gb alakh nasscom dpis digit guillermo dolkens noy pharmacy occupation mindset specialised gccs ensures gxo walking physicswallah bootcamps edtech offset southern derail skillfully\n",
      "Cluster 1 - Topic 6:\n",
      "april noonan responds peggy column danger abstract letter mackenzie openai chatgpt james artificial intelligence generative fte servicenow jain newsroom bargain drescher craig newmark civic imaginative bound fran duncan residual lee\n",
      "Cluster 1 - Topic 7:\n",
      "wayve irish district accent bhooshan beverly bregy expelled incident softbank kendall stipulated vista cotescu disentanglement tinchev phoneme goyal confidential nude masculine linguist brogue prasad tallon newsroom classmate stride circulated corpus\n",
      "\n",
      "Topics for Cluster 2:\n",
      "Cluster 2 - Topic 0:\n",
      "zammitt sosso fm pocket profluent arr luca cafemedia child punchbowl painting viswanathan delangue poshmark dyslexia puck hoffman xai mostaque semafor layer norman band biological skilling microscopic mechanism shoham verge bannister\n",
      "Cluster 2 - Topic 1:\n",
      "cosson victim ema kenner michel identity farmer agriculture theft pouring pe ipad metaphor nasscom draper midcap myhren compressor dhenu chandrasekaran theater mac agricultural fugees rapper unstop karya kissanai miraflor lizard\n",
      "Cluster 2 - Topic 2:\n",
      "bharatgpt shapiro corover mohapatra traded fork projection pillar casey bullish saasboomi macd commonwealth lopresti szuchman lozano eshoo marion mostaque paula hosted davis emad kate saas indicator sbi icici uptake stability\n",
      "Cluster 2 - Topic 3:\n",
      "ai company technology openai new india model image generative google time chatgpt year data use people tool content work meta microsoft one tech intelligence product system apple make artificial language\n",
      "Cluster 2 - Topic 4:\n",
      "dealer rincon ahn searching horwitz dealership schmelkin allegheny chevrolet chevy montemayor hussein alaska baskaran watsonville cooper ford attribute delayed carrier gm barad walkout clothing shopper handeyside cyberwell mohan levine detroit\n",
      "Cluster 2 - Topic 5:\n",
      "pi devin packaging alison stanley lasso morgan sinha sexual greedflation chiplets khosla airbus pizzi thorn willner weber cognition substrate toi coffee aircraft layla healy teton transistor electrical ketatations ketamine madison\n",
      "Cluster 2 - Topic 6:\n",
      "kwon adtech francis cybersecurity dynamic bitcoin pope farhadi vr ar nakasone narrated jain reduces barrier paperwork farmer govern haugh informatics diagnosis schweidel fabrication mishuris facilitate freeing alto palo sinsky diagnosed\n",
      "Cluster 2 - Topic 7:\n",
      "tenen deepfakes child swift huang parscale dasgupta clock etzioni iisc benioff modi accord diffusion doomsday jadoun romero befake ouija abuse immigrant explicit proposal ganesan composer rathore robocalls peds mostaque stability\n",
      "\n",
      "Topics for Cluster 3:\n",
      "Cluster 3 - Topic 0:\n",
      "ai company technology new google openai year india generative chatgpt people model altman work tool data job use time business tech microsoft one product intelligence apple image artificial may service\n",
      "Cluster 3 - Topic 1:\n",
      "chip reliance cognizant japan intel huang semiconductor packaging palantir manufacturing lobbying chiplets helberg supercomputer capgemini award mukesh jio attrition plant kishida apollo jensen matsuno torres ritchie ezzat lobby grace dgx\n",
      "Cluster 3 - Topic 2:\n",
      "walmart wendy dhupar compute mint sourcing gpus yoy killed xi albright exercised penegor purchasing columbus cc gaza kitchen parekh mcmillon lite recovers outpaces nilekani miller household ohio remuneration refinery nest\n",
      "Cluster 3 - Topic 3:\n",
      "genz srm axios tc zvolv vin tsr mediatek vandehei gambling hyperautomation nifty jain league hindenburg silverneedle chipset icahn responsive yellow lcnc median qualcomm taiwanese wise questionnaire rfp rank bearish bull\n",
      "Cluster 3 - Topic 4:\n",
      "student school teacher classroom shuman murray educator ezra jadoun klein trillium walla rathore newfound draper nelson alondra modi teaching class cheating heard bronx unstop karya wearable exam delivering dan grader\n",
      "Cluster 3 - Topic 5:\n",
      "chang japan mongodb maggioncalda tsmc coursera overtime adl tc adverse learner extremism atlas ambee inovaare aomori slipped devnagri ittycheria jeff academy genai yield zomato remote seoul ratio nifty funamizu prefecture\n",
      "Cluster 3 - Topic 6:\n",
      "epic nihilent deeptech xai kansler overblown autonomy singh opus maturing soni grok digitisation ntt unhinged africa depends mahakumbh sweden disagrees sweeney prudential piramal ml hatch philosophical infosys promoter geopolitics subsidiary\n",
      "Cluster 3 - Topic 7:\n",
      "fm hr pocket orkes arr bos orchestration certification nayak dischler conductor freelancer johnny workflow barrie schindler proactive displace gig janardhan sweetnam acknowledge reset advertiser rohan degree strengthen annualised trending sekh\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def display_topics_for_clusters(file_paths, num_top_words, num_topics_per_cluster=8):\n",
    "    for cluster_idx, file_path in enumerate(file_paths):\n",
    "        # Step 1: Load TF-IDF vectors\n",
    "        tfidf_vectors = pd.read_csv(file_path)\n",
    "        tfidf_matrix = csr_matrix(tfidf_vectors.values)\n",
    "\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics_per_cluster, random_state=42)\n",
    "        lda_model.fit(tfidf_matrix)\n",
    "\n",
    "        feature_names = tfidf_vectors.columns\n",
    "        print(f\"\\nTopics for Cluster {cluster_idx}:\")\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            print(f\"Cluster {cluster_idx} - Topic {topic_idx}:\")\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Example usage\n",
    "file_paths = ['cluster0_vectorized.csv', 'cluster1_vectorized.csv', 'cluster2_vectorized.csv', 'cluster3_vectorized.csv']\n",
    "num_top_words = 30\n",
    "display_topics_for_clusters(file_paths, num_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/QuangAP/Library/Python/3.12/lib/python/site-packages (from pyLDAvis) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (1.11.3)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (3.1.4)\n",
      "Requirement already satisfied: numexpr in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (2.10.0)\n",
      "Requirement already satisfied: funcy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyLDAvis) (70.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/QuangAP/Library/Python/3.12/lib/python/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/QuangAP/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame df_topics:\n",
      "             Topic 1            Topic 2  ...            Topic 7            Topic 8\n",
      "0  Cluster 0 Topic 1  Cluster 0 Topic 2  ...  Cluster 0 Topic 7  Cluster 0 Topic 8\n",
      "1  Cluster 1 Topic 1  Cluster 1 Topic 2  ...  Cluster 1 Topic 7  Cluster 1 Topic 8\n",
      "2  Cluster 2 Topic 1  Cluster 2 Topic 2  ...  Cluster 2 Topic 7  Cluster 2 Topic 8\n",
      "3  Cluster 3 Topic 1  Cluster 3 Topic 2  ...  Cluster 3 Topic 7  Cluster 3 Topic 8\n",
      "\n",
      "[4 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data for illustration (replace with your actual topic data)\n",
    "cluster_data = [\n",
    "    ['Cluster 0 Topic 1', 'Cluster 0 Topic 2', 'Cluster 0 Topic 3', 'Cluster 0 Topic 4', \n",
    "     'Cluster 0 Topic 5', 'Cluster 0 Topic 6', 'Cluster 0 Topic 7', 'Cluster 0 Topic 8'],\n",
    "    ['Cluster 1 Topic 1', 'Cluster 1 Topic 2', 'Cluster 1 Topic 3', 'Cluster 1 Topic 4', \n",
    "     'Cluster 1 Topic 5', 'Cluster 1 Topic 6', 'Cluster 1 Topic 7', 'Cluster 1 Topic 8'],\n",
    "    ['Cluster 2 Topic 1', 'Cluster 2 Topic 2', 'Cluster 2 Topic 3', 'Cluster 2 Topic 4', \n",
    "     'Cluster 2 Topic 5', 'Cluster 2 Topic 6', 'Cluster 2 Topic 7', 'Cluster 2 Topic 8'],\n",
    "    ['Cluster 3 Topic 1', 'Cluster 3 Topic 2', 'Cluster 3 Topic 3', 'Cluster 3 Topic 4', \n",
    "     'Cluster 3 Topic 5', 'Cluster 3 Topic 6', 'Cluster 3 Topic 7', 'Cluster 3 Topic 8']\n",
    "]\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_topics = pd.DataFrame(cluster_data, columns=[f\"Topic {i+1}\" for i in range(8)])\n",
    "\n",
    "# Save as CSV\n",
    "df_topics.to_csv('cluster_topics.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame df_topics:\")\n",
    "print(df_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a Mapping_File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     story_ID                                          body_text  cluster\n",
      "0           1  bengaluru infosys solid foundation grow back l...        0\n",
      "1           2  december staff american writer artist institut...        1\n",
      "2           3  bengaluru genz prioritizes professional growth...        2\n",
      "3           4  new technology upend many online business comp...        3\n",
      "4           5  share u tech giant buzzing fund scorching perf...        0\n",
      "..        ...                                                ...      ...\n",
      "495       496  google took next leap artificial intelligence ...        3\n",
      "496       497  abstract startup pouring money generative tech...        0\n",
      "497       498  dozen company popped offer service aimed ident...        1\n",
      "498       499  abstract james mackenzie letter responds peggy...        2\n",
      "499       500  new delhi transformation telecommunication com...        3\n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('tokenizedFine_500.json', 'r') as file:\n",
    "    stories = json.load(file)\n",
    "\n",
    "# Create a unique story_ID for each story\n",
    "story_ids = list(range(1, len(stories) + 1))\n",
    "\n",
    "# Example: Assign clusters to each story\n",
    "# For demonstration, we'll use a simple round-robin assignment\n",
    "num_clusters = 4  # Change this to your actual number of clusters\n",
    "clusters = [i % num_clusters for i in range(len(stories))]\n",
    "\n",
    "# Create the DataFrame\n",
    "mapping_df = pd.DataFrame({\n",
    "    'story_ID': story_ids,\n",
    "    'body_text': stories,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mapping_file_path = 'mapping_file.csv'\n",
    "mapping_df.to_csv(mapping_file_path, index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(mapping_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a DataFrame for Document Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    text_identifier  cluster  topic_0.0  ...  topic_3.5  topic_3.6  topic_3.7\n",
      "0                 1        0   0.005169  ...        NaN        NaN        NaN\n",
      "1                 5        0   0.639780  ...        NaN        NaN        NaN\n",
      "2                 9        0   0.006674  ...        NaN        NaN        NaN\n",
      "3                13        0   0.004694  ...        NaN        NaN        NaN\n",
      "4                17        0   0.007089  ...        NaN        NaN        NaN\n",
      "5                21        0   0.006956  ...        NaN        NaN        NaN\n",
      "6                25        0   0.012360  ...        NaN        NaN        NaN\n",
      "7                29        0        NaN  ...        NaN        NaN        NaN\n",
      "8                33        0        NaN  ...        NaN        NaN        NaN\n",
      "9                37        0        NaN  ...        NaN        NaN        NaN\n",
      "10               41        0        NaN  ...        NaN        NaN        NaN\n",
      "11               45        0        NaN  ...        NaN        NaN        NaN\n",
      "12               49        0        NaN  ...        NaN        NaN        NaN\n",
      "13               53        0        NaN  ...        NaN        NaN        NaN\n",
      "14               57        0        NaN  ...        NaN        NaN        NaN\n",
      "15               61        0        NaN  ...        NaN        NaN        NaN\n",
      "16               65        0        NaN  ...        NaN        NaN        NaN\n",
      "17               69        0        NaN  ...        NaN        NaN        NaN\n",
      "18               73        0        NaN  ...        NaN        NaN        NaN\n",
      "19               77        0        NaN  ...        NaN        NaN        NaN\n",
      "\n",
      "[20 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dataframe_construct(file_paths, mapping_file_path, output_file_path):\n",
    "    # Load the mapping file\n",
    "    mapping_df = pd.read_csv(mapping_file_path)\n",
    "\n",
    "    # Initialize the list to collect rows for the final DataFrame\n",
    "    data = []\n",
    "\n",
    "    # Loop through each cluster\n",
    "    for cluster_idx, file_path in enumerate(file_paths):\n",
    "        # Load the TF-IDF vectors\n",
    "        tfidf_vectors = pd.read_csv(file_path)\n",
    "\n",
    "        # Filter the mapping_df for the current cluster\n",
    "        cluster_mapping = mapping_df[mapping_df['cluster'] == cluster_idx]\n",
    "\n",
    "        # Initialize LDA model for the current cluster\n",
    "        lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "\n",
    "        # Exclude the document identifier column from the TF-IDF matrix\n",
    "        tfidf_matrix = csr_matrix(tfidf_vectors.iloc[:, 1:].values)\n",
    "\n",
    "        lda_model.fit(tfidf_matrix)\n",
    "\n",
    "        # Get topic distributions for each document\n",
    "        topic_distributions = lda_model.transform(tfidf_matrix)\n",
    "\n",
    "        # Add document info and topic distribution to the data list\n",
    "        for idx, row in cluster_mapping.iterrows():\n",
    "            text_identifier = row['story_ID']  # Assuming 'story_ID' is the identifier\n",
    "            row_data = {'text_identifier': text_identifier, 'cluster': cluster_idx}\n",
    "            for topic_idx in range(8):\n",
    "                column_name = f'topic_{cluster_idx}.{topic_idx}'\n",
    "                # Check if idx is within the bounds of topic_distributions\n",
    "                if idx < len(topic_distributions):\n",
    "                    row_data[column_name] = topic_distributions[idx][topic_idx]  # Initialize with topic distribution score\n",
    "                else:\n",
    "                    row_data[column_name] = None  # Handle cases where idx exceeds topic_distributions size\n",
    "            data.append(row_data)\n",
    "\n",
    "    # Create the DataFrame with the collected data\n",
    "    columns = ['text_identifier', 'cluster'] + [f'topic_{i}.{j}' for i in range(len(file_paths)) for j in range(8)]\n",
    "    final_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Save the DataFrame to a CSV file with NaNs replaced by '0.000000000000000000'\n",
    "    final_df.to_csv(output_file_path, index=False, na_rep='0.000000000000000000')\n",
    "\n",
    "    # Print the DataFrame to verify\n",
    "    print(final_df.head(20))\n",
    "\n",
    "# Example usage\n",
    "file_paths = ['cluster0_vectorized.csv', 'cluster1_vectorized.csv', 'cluster2_vectorized.csv', 'cluster3_vectorized.csv']\n",
    "mapping_file_path = 'mapping_file.csv'  # This file should contain \"story_ID\", \"body_text\", and \"cluster\"\n",
    "output_file_path = 'df_topics.csv'\n",
    "\n",
    "# Construct the DataFrame and create df_topics.csv\n",
    "dataframe_construct(file_paths, mapping_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
