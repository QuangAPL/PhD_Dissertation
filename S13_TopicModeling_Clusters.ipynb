{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized cluster saved to cluster0_vectorized.csv\n",
      "Vectorized cluster saved to cluster1_vectorized.csv\n",
      "Vectorized cluster saved to cluster2_vectorized.csv\n",
      "Vectorized cluster saved to cluster3_vectorized.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_tfidf(input_file_paths, output_file_paths, max_features=5000):\n",
    "    for input_file_path, output_file_path in zip(input_file_paths, output_file_paths):\n",
    "        # Load the tokenized data\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            tokenized_data = json.load(file)\n",
    "        \n",
    "        # Assume tokenized_data is a list of strings\n",
    "        # Initialize the TF-IDF Vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "        # Vectorize the tokenized text\n",
    "        tfidf_matrix = vectorizer.fit_transform(tokenized_data)\n",
    "\n",
    "        # Convert to a dense matrix (optional, depending on what you need next)\n",
    "        tfidf_dense = tfidf_matrix.todense()\n",
    "\n",
    "        # Get feature names\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Save as a DataFrame\n",
    "        df_tfidf = pd.DataFrame(tfidf_dense, columns=feature_names)\n",
    "        df_tfidf.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Vectorized cluster saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_paths = ['cluster_0.json', 'cluster_1.json', 'cluster_2.json', 'cluster_3.json']\n",
    "output_file_paths = ['cluster0_vectorized.csv', 'cluster1_vectorized.csv', 'cluster2_vectorized.csv', 'cluster3_vectorized.csv']\n",
    "vectorize_tfidf(input_file_paths, output_file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assigning Topics to Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics for Cluster 0:\n",
      "Cluster 0 - Topic 0:\n",
      "highmark sydelabs wealth barron vulnerability illustrated suggests oracle kumari westfield vertex samtani gas clarke aaa holiday daughter drone arena syde\n",
      "Cluster 0 - Topic 1:\n",
      "agility hanooman shelton digit humanoid holding sml omega prasad cmu robotics seetha stealth hurst warehouse alumnus entity cassie massage bipedal\n",
      "Cluster 0 - Topic 2:\n",
      "chang tc tsmc murphy stanley morgan governor jersey recruitment pizzi grows adl extremism shareholder abortion audit volatility chandrasekaran wealth evil\n",
      "Cluster 0 - Topic 3:\n",
      "ai company technology new openai google chatgpt image india year tool generative artificial intelligence model one use microsoft time people\n",
      "Cluster 0 - Topic 4:\n",
      "japan desantis chandrasekhar responds noonan fcc robocalls concludes therefore exposed tom arpu telco telecom deepfakes farid hat sayeed thorn kishida\n",
      "Cluster 0 - Topic 5:\n",
      "drug peds athlete mit carpenter kellis xie doping cheat ped incurable molecular hurdle hunter fda alter undetectable discovered prove practical\n",
      "Cluster 0 - Topic 6:\n",
      "contends andy range flashcard chablani felfel busta eshoo schreppel ingber corey shannon hanna mahmoud mahima heikkil heel studying garmin cardio\n",
      "Cluster 0 - Topic 7:\n",
      "grime lensa hang fusion capgemini marketer oral cognizant affected discourage reduce selfies grimesai locket carbon prisma featuring fairy fat beehero\n",
      "\n",
      "Topics for Cluster 1:\n",
      "Cluster 1 - Topic 0:\n",
      "henry drawing fusion mediatek ravinutala doc gmail daniel pandora schmidt qualcomm workspace eric logits purdue sujith locket flagship column divorce\n",
      "Cluster 1 - Topic 1:\n",
      "gene plastic profluent crispr dealer biological microscopic edit mechanism horwitz dealership westfield dna learns sourcing mop chevy bratton chevrolet acid\n",
      "Cluster 1 - Topic 2:\n",
      "ai company technology new india google apple openai year time job microsoft data generative chatgpt people altman work one use\n",
      "Cluster 1 - Topic 3:\n",
      "tenen cruise zimmermann bag ftc hindenburg cc passenger driverless erin script cinema matching firefly footage section admission icahn runway turmoil\n",
      "Cluster 1 - Topic 4:\n",
      "patent alison blizzard iowa abbott reel oxygen masimo moines blood container sullivan nude invent ren inventor pornography thaler ultra cc\n",
      "Cluster 1 - Topic 5:\n",
      "inflection patient doctor pi hoffman iisc hospital suleyman military laguna beach ganesan diagnosis weapon cd iit gallagher ahn niue computational\n",
      "Cluster 1 - Topic 6:\n",
      "sora bharatgpt reben photography corover commercial gallagher candle painting photoshop residence camp surged photographic cookie generic simmons cartagena juice pizza\n",
      "Cluster 1 - Topic 7:\n",
      "ernie orth rephrase kansler composer laidlow wang schankler rap orchestra traveling tallon lewis aesthetic synthesized sang forager duke classical musiclm\n",
      "\n",
      "Topics for Cluster 2:\n",
      "Cluster 2 - Topic 0:\n",
      "tenen zoom office india ai supplier tata walmart consent nvidia sourcing infrastructure book huang data adobe creative application exemption content\n",
      "Cluster 2 - Topic 1:\n",
      "mckinsey child care mint beer bos consulting daily interview swift singla consultant group inflation taylor walmart money today usa tax\n",
      "Cluster 2 - Topic 2:\n",
      "reben model student artist language boeing school merchat art shop teacher chatgpt chatbots chatbot chair understand tool neural layer hybrid\n",
      "Cluster 2 - Topic 3:\n",
      "china getty salle united artist report image ukraine mckinsey researcher chinese drake art sleeve music painting peter musician heart algorithm\n",
      "Cluster 2 - Topic 4:\n",
      "abstract april openai repair column ai microsoft adobe chip power altman india graphic photo board rephrase generative declaration james developer\n",
      "Cluster 2 - Topic 5:\n",
      "minor explicit sexually conducted dean craven girl revolutionize physician sexual neural healthcare stanford delivered journal school discus deepfake october everything\n",
      "Cluster 2 - Topic 6:\n",
      "openai company product tech doctor bank copyright deal review ai genai time amazon microsoft generative patient chatbot soaring new ramani\n",
      "Cluster 2 - Topic 7:\n",
      "ai company technology data new one people apple think human work make state intelligence use year model industry service generative\n",
      "\n",
      "Topics for Cluster 3:\n",
      "Cluster 3 - Topic 0:\n",
      "reddit ai microsoft hospital google company tech product doctor patient business system data tool new technology conversation algorithm list guide\n",
      "Cluster 3 - Topic 1:\n",
      "epic google capillary secondary million exit app store antitrust fortnite company equity investor reddy funding game raise transaction existing round\n",
      "Cluster 3 - Topic 2:\n",
      "ar summary investor venture ai snap pose poll funding technology company million leader page tech potential mobius cohere four people\n",
      "Cluster 3 - Topic 3:\n",
      "chandok clinical ai india trial llm healthcare data hospital customer language enterprise company model adoption healthgpt vardhan harman build digital\n",
      "Cluster 3 - Topic 4:\n",
      "trade global sternfels growth smes inclusive trillion mckinsey flow massive inclusion route today economic gdp resilient gender accelerating protectionism couple\n",
      "Cluster 3 - Topic 5:\n",
      "cognizant kumar ceo revenue leadership margin wipro back deal infosys growth market ravi firm service billion et talent competitor senior\n",
      "Cluster 3 - Topic 6:\n",
      "sutskever altman openai startup ai india board innovation compute gpus company mission pachocki investment billion access provide uk build application\n",
      "Cluster 3 - Topic 7:\n",
      "microsoft union company copyright worker nurse activision office tester labor smith time employee tech work hospital report amazon gensler combination\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def display_topics_for_clusters(file_paths, num_top_words, num_topics_per_cluster=8):\n",
    "    for cluster_idx, file_path in enumerate(file_paths):\n",
    "        # Step 1: Load TF-IDF vectors\n",
    "        tfidf_vectors = pd.read_csv(file_path)\n",
    "        tfidf_matrix = csr_matrix(tfidf_vectors.values)\n",
    "\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics_per_cluster, random_state=42)\n",
    "        lda_model.fit(tfidf_matrix)\n",
    "\n",
    "        feature_names = tfidf_vectors.columns\n",
    "        print(f\"\\nTopics for Cluster {cluster_idx}:\")\n",
    "        for topic_idx, topic in enumerate(lda_model.components_):\n",
    "            print(f\"Cluster {cluster_idx} - Topic {topic_idx}:\")\n",
    "            print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "# Example usage\n",
    "file_paths = ['cluster0_vectorized.csv', 'cluster1_vectorized.csv', 'cluster2_vectorized.csv', 'cluster3_vectorized.csv']\n",
    "num_top_words = 20\n",
    "display_topics_for_clusters(file_paths, num_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pyLDAvis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
