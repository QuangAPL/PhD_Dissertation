{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering Corpus by Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563 news stories from The New York Times saved to NYTimes_stories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to filter news stories by media and save to a new file\n",
    "def filter_by_media(input_file_path, media_name, output_file_path):\n",
    "    # Load data from the input file\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        news_stories = json.load(f)\n",
    "\n",
    "    # Filter news stories by media\n",
    "    filtered_stories = [story for story in news_stories if story['media'] == media_name]\n",
    "\n",
    "    # Save the filtered stories to a new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_stories, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    num_filtered_stories = len(filtered_stories)\n",
    "    \n",
    "    print(f\"{num_filtered_stories} news stories from {media_name} saved to {output_file_path}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = 'clean_corpus.json'\n",
    "output_file_path = 'NYTimes_stories.json'\n",
    "media_name = \"The New York Times\"\n",
    "\n",
    "filter_by_media(input_file_path, media_name, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering Corpus by a Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 781 stories containing the keyword 'ChatGPT'.\n",
      "Filtered stories saved to: ChatGPT_corpus.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def def_keyword_filter(input_file_path, keyword):\n",
    "    # Load cleaned corpus from JSON file\n",
    "    def load_clean_corpus(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    # Filter stories containing the specified keyword\n",
    "    def filter_stories_by_keyword(data, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        filtered_stories = [item for item in data if keyword_lower in item['story_text'].lower()]\n",
    "        return filtered_stories\n",
    "\n",
    "    # Save filtered stories to a new JSON file\n",
    "    def save_filtered_stories(filtered_stories, output_file_path):\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_stories, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Define output file path\n",
    "    output_file_path = 'ChatGPT_corpus.json'\n",
    "\n",
    "    # Load and filter stories\n",
    "    data = load_clean_corpus(input_file_path)\n",
    "    filtered_stories = filter_stories_by_keyword(data, keyword)\n",
    "\n",
    "    # Save the filtered stories\n",
    "    save_filtered_stories(filtered_stories, output_file_path)\n",
    "\n",
    "    print(f\"Filtered {len(filtered_stories)} stories containing the keyword '{keyword}'.\")\n",
    "    \n",
    "    return output_file_path\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'clean_corpus.json'\n",
    "keyword = 'ChatGPT'\n",
    "output_file_path = def_keyword_filter(input_file_path, keyword)\n",
    "print(f\"Filtered stories saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering Corpus by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 150 stories released in March 2024.\n",
      "Filtered stories saved to: Mar2024_corpus.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def def_filter(input_file_path, target_month_str, target_year):\n",
    "    # Load cleaned corpus from JSON file\n",
    "    def load_clean_corpus(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    # Filter stories released in the specified month and year\n",
    "    def filter_stories_by_date(data, target_month_str, target_year):\n",
    "        filtered_stories = []\n",
    "        for item in data:\n",
    "            if item['time'] != \"N/A\":\n",
    "                try:\n",
    "                    story_date = datetime.strptime(item['time'], '%B %d, %Y')\n",
    "                    if story_date.strftime('%B') == target_month_str and story_date.year == target_year:\n",
    "                        filtered_stories.append(item)\n",
    "                except ValueError:\n",
    "                    pass  # Skip processing for invalid date formats or \"N/A\"\n",
    "        return filtered_stories\n",
    "\n",
    "    # Save filtered stories to a new JSON file\n",
    "    def save_filtered_stories(filtered_stories, output_file_path):\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_stories, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Define output file path\n",
    "    output_file_path = 'Mar2024_corpus.json'\n",
    "\n",
    "    # Load and filter stories\n",
    "    data = load_clean_corpus(input_file_path)\n",
    "    filtered_stories = filter_stories_by_date(data, target_month_str, target_year)\n",
    "\n",
    "    # Save the filtered stories\n",
    "    save_filtered_stories(filtered_stories, output_file_path)\n",
    "\n",
    "    print(f\"Filtered {len(filtered_stories)} stories released in {target_month_str} {target_year}.\")\n",
    "    \n",
    "    return output_file_path\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'clean_corpus.json'\n",
    "target_month_str = 'March'\n",
    "target_year = 2024\n",
    "output_file_path = def_filter(input_file_path, target_month_str, target_year)\n",
    "print(f\"Filtered stories saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging all PDF Files into  one PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def merge_pdf_files(directory_path, output_file_path):\n",
    "    # Initialize PDF writer\n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "    \n",
    "    # List PDF files in the directory\n",
    "    pdf_files = [file for file in os.listdir(directory_path) if file.endswith('.pdf')]\n",
    "    \n",
    "    # Sort PDF files by name\n",
    "    pdf_files.sort()\n",
    "    \n",
    "    # Iterate over PDF files and merge all pages from each file\n",
    "    for pdf_file in pdf_files:\n",
    "        with open(os.path.join(directory_path, pdf_file), 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            # Merge all pages\n",
    "            for page in pdf_reader.pages:\n",
    "                pdf_writer.add_page(page)\n",
    "    \n",
    "    # Write the merged PDF to the output file\n",
    "    with open(output_file_path, 'wb') as output_file:\n",
    "        pdf_writer.write(output_file)\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni'  # Replace with the path to your directory containing PDF files\n",
    "output_file_path = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni/merged_corpus.pdf'    # Replace with the desired output PDF file path\n",
    "merge_pdf_files(directory_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating Corpus to Individual PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def separate_news_stories(input_file_path, output_directory):\n",
    "    # Initialize PDF reader\n",
    "    pdf_reader = PyPDF2.PdfReader(input_file_path)\n",
    "    \n",
    "    # Get the total number of pages in the PDF\n",
    "    total_pages = len(pdf_reader.pages)\n",
    "    \n",
    "    # Initialize variables for tracking news stories\n",
    "    start_page = 0\n",
    "    story_num = 1\n",
    "    \n",
    "    # Iterate through each page to identify and separate news stories\n",
    "    for page_num in range(total_pages):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        if \"End of Document\" in page_text:\n",
    "            # Extract the news story between start_page and page_num\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            for i in range(start_page, page_num + 1):\n",
    "                pdf_writer.add_page(pdf_reader.pages[i])\n",
    "            # Write the news story to a separate PDF file\n",
    "            output_file_path = os.path.join(output_directory, f'news_story_{story_num}.pdf')\n",
    "            with open(output_file_path, 'wb') as output_file:\n",
    "                pdf_writer.write(output_file)\n",
    "            # Update variables for the next news story\n",
    "            start_page = page_num + 1\n",
    "            story_num += 1\n",
    "    \n",
    "    # Check if there are any remaining pages after the last news story\n",
    "    if start_page < total_pages:\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "        for i in range(start_page, total_pages):\n",
    "            pdf_writer.add_page(pdf_reader.pages[i])\n",
    "        # Write the remaining pages to a separate PDF file\n",
    "        output_file_path = os.path.join(output_directory, f'news_story_{story_num}.pdf')\n",
    "        with open(output_file_path, 'wb') as output_file:\n",
    "            pdf_writer.write(output_file)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni/merged_corpus.pdf'  # Replace with the path to your PDF corpus\n",
    "output_directory = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni/separated_stories'  # Replace with the directory to save separated news stories\n",
    "separate_news_stories(input_file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling 50 Single Stories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def sampling_50(input_directory, sample_num, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # List all files in the input directory\n",
    "    all_files = os.listdir(input_directory)\n",
    "    # Filter out non-PDF files\n",
    "    pdf_files = [file for file in all_files if file.endswith('.pdf')]\n",
    "    \n",
    "    # Randomly sample 50 files\n",
    "    sampled_files = random.sample(pdf_files, min(sample_num, len(pdf_files)))\n",
    "    \n",
    "    # Copy the sampled files to the output directory\n",
    "    for file_name in sampled_files:\n",
    "        src_file_path = os.path.join(input_directory, file_name)\n",
    "        dst_file_path = os.path.join(output_directory, file_name)\n",
    "        shutil.copyfile(src_file_path, dst_file_path)\n",
    "\n",
    "# Example usage\n",
    "input_directory = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni/separated_stories'  # Replace with the path to your separated stories directory\n",
    "sample_num = 50  # Number of stories to sample\n",
    "output_directory = '/Users/QuangAP/Quang_Apollo/Dissertation_Data/NexisUni/50sample_stories'  # Replace with the path to your output directory\n",
    "sampling_50(input_directory, sample_num, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames have been saved to framed_200.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def determine_frame(input_file_path, frame_path, output_file_path):\n",
    "    # Function to read frames from files\n",
    "    def load_frames(directory):\n",
    "        frames = {}\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                frame_name = filename.rsplit('.', 1)[0]\n",
    "                with open(os.path.join(directory, filename), 'r') as file:\n",
    "                    keywords = [line.strip().lower() for line in file if line.strip()]\n",
    "                frames[frame_name] = keywords\n",
    "        return frames\n",
    "\n",
    "    # Function to determine the frame of a document\n",
    "    def get_frame(tokenized_doc, frames):\n",
    "        frame_scores = {frame: 0 for frame in frames}\n",
    "        \n",
    "        for word in tokenized_doc:\n",
    "            for frame, keywords in frames.items():\n",
    "                if word in keywords:\n",
    "                    frame_scores[frame] += 1\n",
    "        \n",
    "        # Return the frame with the highest score\n",
    "        return max(frame_scores, key=frame_scores.get)\n",
    "\n",
    "    # Load frames from the directory\n",
    "    frames = load_frames(frame_path)\n",
    "\n",
    "    # Load tokenized stories from JSON file\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        tokenized_stories = json.load(file)\n",
    "\n",
    "    # Determine the frame of each document in the corpus\n",
    "    document_frames = []\n",
    "    for document in tokenized_stories:\n",
    "        frame = get_frame(document, frames)\n",
    "        document_frames.append(frame)\n",
    "\n",
    "    # Save the frames results into a new JSON file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(document_frames, file)\n",
    "\n",
    "    print(f\"Frames have been saved to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'preprocessed_200.json'\n",
    "frame_path = '/Users/QuangAP/Quang_Apollo/Frames'\n",
    "output_file_path = 'framed_200.json'\n",
    "\n",
    "determine_frame(input_file_path, frame_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
